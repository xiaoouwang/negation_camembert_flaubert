{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does CamemBERT/FlauBERT/Bert understand negation?\n",
    "\n",
    "Short answer: no.\n",
    "\n",
    "This notebook replicates a section of the paper of `Ettinger2020` on negation using a French corpus. The idea is to add negation on propositions and test if Bert `without fine-tuning` is able to switch the answer from one to another.\n",
    "\n",
    "Actually this principle resembles a lot the Winograd Schema Challenge.\n",
    "\n",
    "Here is an example in French:\n",
    "\n",
    "```\n",
    "id;masked;tgt;item;cond;right_answer;options\n",
    "0;La truite est un <mask>;poisson;0;TA;poisson;poisson outil\n",
    "1;La truite n'est pas un <mask>;poisson;0;FN;outil;poisson outil\n",
    "```\n",
    "The `<mask>` token should be `poisson` in the first example and `outil` in the second.\n",
    "\n",
    "However, in a corpus of 18 pairs of sentences Bert was unable to switch response on either pair.\n",
    "\n",
    "There's no reason for CamemBERT/FlauBERT to behave differently on this task.\n",
    "\n",
    "This notebook shows that French counterparts of Bert (CamemBERT and FlauBERT) are indeed incapable of performing better on a French corpus.\n",
    "\n",
    "Please note that I write some wrapper functions to faciliate use of French Berts published via a packge named `frenchnlp`. Be sure to install it before running this notebook on your computer.\n",
    "\n",
    "`!pip install frenchnlp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions\n",
    "\n",
    "from frenchnlp import *\n",
    "\n",
    "def change_verb(sent,new_verb):\n",
    "    return sent.replace(\"n'est\",\"ne \"+ new_verb).replace(\"est\",new_verb)\n",
    "\n",
    "def change_header(sent,new_term):\n",
    "    if \"La\" in sent:\n",
    "        return sent.replace(\"La\",\"Le terme\")\n",
    "    else:\n",
    "        return sent.replace(\"Le\",\"Le terme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus and variations\n",
    "\n",
    "I translated the items of Ettinger2020 into French. Besides, I add some variations to test the effect of minor modifications on the original utterance.\n",
    "\n",
    "For a sentence like `La truite est un <mask>`, you have:\n",
    "\n",
    "* La truite représente un `<mask>`.\n",
    "* La truite représente un `<mask>` en français.\n",
    "* Le terme truite désigne un `<mask>`.\n",
    "* Le terme truite désigne un `<mask>` en français.\n",
    "\n",
    "I append `en français` to each sentence because it is possible that Bert works better when the left and right contexts are provided since it was not trained like GPT3 on a traditional language modeling task.\n",
    "\n",
    "As you can see from the dataframe, `<mask>` was also replaced with `<special1>` to comply with FlauBERT's annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>masked</th>\n",
       "      <th>tgt</th>\n",
       "      <th>item</th>\n",
       "      <th>cond</th>\n",
       "      <th>right_answer</th>\n",
       "      <th>options</th>\n",
       "      <th>response1</th>\n",
       "      <th>prob1</th>\n",
       "      <th>response2</th>\n",
       "      <th>...</th>\n",
       "      <th>masked_cam</th>\n",
       "      <th>masked_flau</th>\n",
       "      <th>ch_verb</th>\n",
       "      <th>ch_verb_flau</th>\n",
       "      <th>ch_verb_add_right</th>\n",
       "      <th>ch_verb_add_right_flau</th>\n",
       "      <th>ch_whole</th>\n",
       "      <th>ch_whole_flau</th>\n",
       "      <th>ch_whole_add_right</th>\n",
       "      <th>ch_whole_add_right_flau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>La truite est un &lt;mask&gt;</td>\n",
       "      <td>poisson</td>\n",
       "      <td>0</td>\n",
       "      <td>TA</td>\n",
       "      <td>poisson</td>\n",
       "      <td>poisson outil</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>La truite est un &lt;mask&gt;.</td>\n",
       "      <td>La truite est un &lt;special1&gt;.</td>\n",
       "      <td>La truite représente un &lt;mask&gt;.</td>\n",
       "      <td>La truite représente un &lt;special1&gt;.</td>\n",
       "      <td>La truite représente un &lt;mask&gt; en français.</td>\n",
       "      <td>La truite représente un &lt;special1&gt; en français.</td>\n",
       "      <td>Le terme truite désigne un &lt;mask&gt;.</td>\n",
       "      <td>Le terme truite désigne un &lt;special1&gt;.</td>\n",
       "      <td>Le terme truite désigne un &lt;mask&gt; en français.</td>\n",
       "      <td>Le terme truite désigne un &lt;special1&gt; en franç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>La truite n'est pas un &lt;mask&gt;</td>\n",
       "      <td>poisson</td>\n",
       "      <td>0</td>\n",
       "      <td>FN</td>\n",
       "      <td>outil</td>\n",
       "      <td>poisson outil</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>La truite n'est pas un &lt;mask&gt;.</td>\n",
       "      <td>La truite n'est pas un &lt;special1&gt;.</td>\n",
       "      <td>La truite ne représente pas un &lt;mask&gt;.</td>\n",
       "      <td>La truite ne représente pas un &lt;special1&gt;.</td>\n",
       "      <td>La truite ne représente pas un &lt;mask&gt; en franç...</td>\n",
       "      <td>La truite ne représente pas un &lt;special1&gt; en f...</td>\n",
       "      <td>Le terme truite ne désigne pas un &lt;mask&gt;.</td>\n",
       "      <td>Le terme truite ne désigne pas un &lt;special1&gt;.</td>\n",
       "      <td>Le terme truite ne désigne pas un &lt;mask&gt; en fr...</td>\n",
       "      <td>Le terme truite ne désigne pas un &lt;special1&gt; e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                         masked      tgt item cond right_answer  \\\n",
       "0  0        La truite est un <mask>  poisson    0   TA      poisson   \n",
       "1  1  La truite n'est pas un <mask>  poisson    0   FN        outil   \n",
       "\n",
       "         options  response1  prob1  response2  ...  \\\n",
       "0  poisson outil          0      0          0  ...   \n",
       "1  poisson outil          0      0          0  ...   \n",
       "\n",
       "                       masked_cam                         masked_flau  \\\n",
       "0        La truite est un <mask>.        La truite est un <special1>.   \n",
       "1  La truite n'est pas un <mask>.  La truite n'est pas un <special1>.   \n",
       "\n",
       "                                  ch_verb  \\\n",
       "0         La truite représente un <mask>.   \n",
       "1  La truite ne représente pas un <mask>.   \n",
       "\n",
       "                                 ch_verb_flau  \\\n",
       "0         La truite représente un <special1>.   \n",
       "1  La truite ne représente pas un <special1>.   \n",
       "\n",
       "                                   ch_verb_add_right  \\\n",
       "0        La truite représente un <mask> en français.   \n",
       "1  La truite ne représente pas un <mask> en franç...   \n",
       "\n",
       "                              ch_verb_add_right_flau  \\\n",
       "0    La truite représente un <special1> en français.   \n",
       "1  La truite ne représente pas un <special1> en f...   \n",
       "\n",
       "                                    ch_whole  \\\n",
       "0         Le terme truite désigne un <mask>.   \n",
       "1  Le terme truite ne désigne pas un <mask>.   \n",
       "\n",
       "                                   ch_whole_flau  \\\n",
       "0         Le terme truite désigne un <special1>.   \n",
       "1  Le terme truite ne désigne pas un <special1>.   \n",
       "\n",
       "                                  ch_whole_add_right  \\\n",
       "0     Le terme truite désigne un <mask> en français.   \n",
       "1  Le terme truite ne désigne pas un <mask> en fr...   \n",
       "\n",
       "                             ch_whole_add_right_flau  \n",
       "0  Le terme truite désigne un <special1> en franç...  \n",
       "1  Le terme truite ne désigne pas un <special1> e...  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = xo_load_data(\"negation_french.csv\")\n",
    "df[\"masked_cam\"] = df[\"masked\"].apply(lambda x:x+\".\")\n",
    "df[\"masked_flau\"] = df[\"masked_cam\"].apply(lambda x:x.replace(\"<mask>\",\"<special1>\")) \n",
    "df[\"ch_verb\"] = df[\"masked\"].apply(lambda x: change_verb(x,\"représente\")+\".\")\n",
    "df[\"ch_verb_flau\"] = df[\"masked\"].apply(lambda x:change_verb(x,\"représente\").replace(\"<mask>\",\"<special1>\")+\".\")\n",
    "df[\"ch_verb_add_right\"] = df[\"ch_verb\"].apply(lambda x:x.replace(\".\",\"\")+\" en français.\")\n",
    "df[\"ch_verb_add_right_flau\"] = df[\"ch_verb_flau\"].apply(lambda x:x.replace(\".\",\"\")+\" en français.\")\n",
    "df[\"ch_whole\"] = df[\"masked_cam\"].apply(lambda x:change_verb(change_header(x,\"Le terme\"),\"désigne\"))\n",
    "df[\"ch_whole_flau\"] = df[\"masked_flau\"].apply(lambda x:change_verb(change_header(x,\"Le terme\"),\"désigne\"))\n",
    "df[\"ch_whole_add_right\"] = df[\"ch_whole\"].apply(lambda x:x.replace(\".\",\"\")+\" en français.\")\n",
    "df[\"ch_whole_add_right_flau\"] = df[\"ch_whole_flau\"].apply(lambda x:x.replace(\".\",\"\")+\" en français.\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FlaubertWithLMHeadModel were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Camembert\n",
    "pipeline = xo_fillin(\"camembert-base\",1000)\n",
    "cam_results = xo_produce_answers(pipeline,\"masked_cam\",df)\n",
    "\n",
    "# Flaubert\n",
    "pipeline_flau = xo_fillin(\"flaubert/flaubert_base_cased\",1000)\n",
    "flau_results = xo_produce_answers(pipeline_flau,\"masked_flau\",df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on the standard version with no variation\n",
    "\n",
    "For examples like `La truite est un <mask>`, note that the accuracy is 50% with equal number of right and wrong responses.\n",
    "\n",
    "Interestingly, both models behaved the same way.\n",
    "\n",
    "This is somewhat expected because both models were trained on similar corpus using similar method. Some differences exist, however. CamemBERT was trained using `whole word masking` and FlauBERT `token masking`. For details, please read the original papers.\n",
    "\n",
    "Suppose that no switch ever happens, the accuracy should be exactly 50%. However upon closer investigation, a successful switch takes place for CamemBERT.\n",
    "\n",
    "```\n",
    "La fourmi est un <mask>. insecte\n",
    "La fourmi n'est pas un <mask>. légume\n",
    "```\n",
    "\n",
    "The 50% accuracy is due to another switch with both wrong answers.\n",
    "\n",
    "```\n",
    "Le petit pois est un <mask>.\t bâtiment\t\n",
    "Le petit pois n'est pas un <mask>.\t légume\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   correct  no_response  bad_response  total_responses  exactitude  qualite  \\\n",
      "0       18            0            18               36        50.0     50.0   \n",
      "\n",
      "   reussite  \n",
      "0      50.0  \n",
      "   correct  no_response  bad_response  total_responses  exactitude  qualite  \\\n",
      "0       18            0            18               36        50.0     50.0   \n",
      "\n",
      "   reussite  \n",
      "0      50.0  \n"
     ]
    }
   ],
   "source": [
    "# 50% in both cases\n",
    "print(xo_compute_score(\"right_answer\",cam_results))\n",
    "print(xo_compute_score(\"right_answer\",flau_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_results[[\"masked_cam\",\"options\",\"response1\",\"prob1\",\"response2\",\"prob2\"]].to_csv(\"res_cam1.csv\",index=False)\n",
    "flau_results[[\"masked_flau\",\"options\",\"response1\",\"prob1\",\"response2\",\"prob2\"]].to_csv(\"res_flau1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on the version with être replaced by représente\n",
    "\n",
    "The accuracy decreases mainly because of the asymmetry between responses for affirmative items and those for negative items. Put in simpler terms, in some cases CamemBERT/FlauBERT gave only one answer to a pair of sentences and this one answer was wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FlaubertWithLMHeadModel were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "   correct  no_response  bad_response  total_responses  exactitude  qualite  \\\n",
      "0       15            3            18               36       41.67    45.45   \n",
      "\n",
      "   reussite  \n",
      "0     45.83  \n",
      "   correct  no_response  bad_response  total_responses  exactitude  qualite  \\\n",
      "0       15            3            18               36       41.67    45.45   \n",
      "\n",
      "   reussite  \n",
      "0     45.83  \n"
     ]
    }
   ],
   "source": [
    "pipeline = xo_fillin(\"camembert-base\",1000)\n",
    "cam_results_ch_verb = xo_produce_answers(pipeline,\"ch_verb\",df)\n",
    "pipeline_flau = xo_fillin(\"flaubert/flaubert_base_cased\",1000)\n",
    "flau_results_ch_verb = xo_produce_answers(pipeline_flau,\"ch_verb_flau\",df)\n",
    "print(xo_compute_score(\"right_answer\",cam_results_ch_verb))\n",
    "print(xo_compute_score(\"right_answer\",flau_results_ch_verb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_results_ch_verb[[\"ch_verb\",\"options\",\"response1\",\"prob1\",\"response2\",\"prob2\"]].to_csv(\"res_cam2.csv\",index=False)\n",
    "flau_results_ch_verb[[\"ch_verb_flau\",\"options\",\"response1\",\"prob1\",\"response2\",\"prob2\"]].to_csv(\"res_flau2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on the version with être replaced by désigner\n",
    "\n",
    "The accuracy decreases further for the same reason mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FlaubertWithLMHeadModel were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "   correct  no_response  bad_response  total_responses  exactitude  qualite  \\\n",
      "0       11           13            12               36       30.56    47.83   \n",
      "\n",
      "   reussite  \n",
      "0     48.61  \n",
      "   correct  no_response  bad_response  total_responses  exactitude  qualite  \\\n",
      "0       11           13            12               36       30.56    47.83   \n",
      "\n",
      "   reussite  \n",
      "0     48.61  \n"
     ]
    }
   ],
   "source": [
    "pipeline = xo_fillin(\"camembert-base\",1000)\n",
    "cam_results_ch_verb_add_right = xo_produce_answers(pipeline,\"ch_verb_add_right\",df)\n",
    "pipeline_flau = xo_fillin(\"flaubert/flaubert_base_cased\",1000)\n",
    "flau_results_ch_verb_add_right_flau = xo_produce_answers(pipeline_flau,\"ch_verb_add_right_flau\",df)\n",
    "# 50% in both cases\n",
    "print(xo_compute_score(\"right_answer\",cam_results_ch_verb_add_right))\n",
    "print(xo_compute_score(\"right_answer\",flau_results_ch_verb_add_right_flau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_results_ch_verb_add_right[[\"ch_verb_add_right\",\"options\",\"response1\",\"prob1\",\"response2\",\"prob2\"]].to_csv(\"res_cam3.csv\",index=False)\n",
    "flau_results_ch_verb_add_right_flau[[\"ch_verb_add_right_flau\",\"options\",\"response1\",\"prob1\",\"response2\",\"prob2\"]].to_csv(\"res_flau3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on the version with être replaced by désigner and a right context\n",
    "\n",
    "The accuracy decreases further more for the same reason mentioned earlier. Also note the great number of non responses. This showes that how the utterance is phrased has an effect on the responses. \n",
    "\n",
    "Note that in this case, the `qualite` measure considering only the answered sentences is more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FlaubertWithLMHeadModel were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "   correct  no_response  bad_response  total_responses  exactitude  qualite  \\\n",
      "0        9           20             7               36        25.0    56.25   \n",
      "\n",
      "   reussite  \n",
      "0     52.78  \n",
      "   correct  no_response  bad_response  total_responses  exactitude  qualite  \\\n",
      "0        9           20             7               36        25.0    56.25   \n",
      "\n",
      "   reussite  \n",
      "0     52.78  \n"
     ]
    }
   ],
   "source": [
    "pipeline = xo_fillin(\"camembert-base\",1000)\n",
    "cam_results_ch_whole_add_right = xo_produce_answers(pipeline,\"ch_whole_add_right\",df)\n",
    "pipeline_flau = xo_fillin(\"flaubert/flaubert_base_cased\",1000)\n",
    "flau_results_ch_whole_add_right_flau = xo_produce_answers(pipeline_flau,\"ch_whole_add_right_flau\",df)\n",
    "# 50% in both cases\n",
    "print(xo_compute_score(\"right_answer\",cam_results_ch_whole_add_right))\n",
    "print(xo_compute_score(\"right_answer\",flau_results_ch_whole_add_right_flau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_results_ch_whole_add_right[[\"ch_whole_add_right\",\"options\",\"response1\",\"prob1\",\"response2\",\"prob2\"]].to_csv(\"res_cam4.csv\",index=False)\n",
    "flau_results_ch_whole_add_right_flau[[\"ch_whole_add_right_flau\",\"options\",\"response1\",\"prob1\",\"response2\",\"prob2\"]].to_csv(\"res_flau4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This simple experiment shows the insensitivity of Bert-like language models to negation. The same observations have proven to be valid on a similar French corpus.\n",
    "\n",
    "What improvements can be made?\n",
    "\n",
    "The addition of self-supervised tasks requiring more sophisticated linguistic information than the simple linear order (such as the addition of syntactic information by (Xu et al., 2020)) could be helpful.\n",
    "\n",
    "## Reference\n",
    "\n",
    "Devlin, J., Chang M.-W., Lee K. and Toutanova K. (2019). “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” In NAACL-HLT 2019.\n",
    "\n",
    "Ettinger, A. (2019). What bert is not : Lessons from a new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:34–48.\n",
    "\n",
    "Le, H., Vial, L., Frej, J., Segonne, V., Coavoux, M., Lecouteux, B., ... & Schwab, D. (2019). Flaubert: Unsupervised language model pre-training for french. arXiv preprint arXiv:1912.05372.\n",
    "\n",
    "Martin, L., Muller, B., Suárez, P. J. O., Dupont, Y., Romary, L., de la Clergerie, É. V., ... & Sagot, B. (2019). Camembert: a tasty french language model. arXiv preprint arXiv:1911.03894.\n",
    "\n",
    "Xu Z., Guo D., Tang D., Su Q., Shou L., Gong M., Zhong W., Quan X., Duan N., and Jiang D. (2020) “Syntax-Enhanced Pre-trained Model”. arXiv:2012.14116\n"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/c3a0968fbad83f884d6db95b7b4c96d0"
  },
  "gist": {
   "data": {
    "description": "bert_negation.ipynb",
    "public": true
   },
   "id": "c3a0968fbad83f884d6db95b7b4c96d0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table des matières",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
